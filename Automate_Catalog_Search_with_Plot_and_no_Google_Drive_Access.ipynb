{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNc9A97TiF+tVyOIgUe8nM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lonestarchaser/SED/blob/main/Automate_Catalog_Search_with_Plot_and_no_Google_Drive_Access.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's Read a Table into IRSA Catalog Search Tool, Make a Match, and Make a Plot\n",
        "\n",
        "Find the gj.tbl file. This is the Gliese-Jareiss catalog of nearby stars. All of them should be pretty good coordinates; the positions come from https://ui.adsabs.harvard.edu/abs/2010PASP..122..885S/abstract\n",
        "\n",
        "This is our goal, but to do it in Python:\n",
        "\n",
        "* Go to IRSA Catalog Search Tool\n",
        "* Pick Gaia DR3\n",
        "* Do a multi-object search\n",
        "  * 1-to-1 matching\n",
        "  * 2 arcsecond radius\n",
        "* Upload the gj.tbl list of sources\n",
        "\n",
        "Some acronyms you might like to know:\n",
        "* ADQL = Astronomical Data Query Language\n",
        "* TAP = Table Access Protocol\n"
      ],
      "metadata": {
        "id": "UqsIdfQzowGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First, install Astropy libraries and import packages and subpackages\n",
        "These aren't normally included in Colab. You can save yourself some time and only have to run this once per time you open this."
      ],
      "metadata": {
        "id": "5SrnJTvbozjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUdlqnQHopTK"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install astroquery gala"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import subpackages\n"
      ],
      "metadata": {
        "id": "gwXwYxFJpN8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import requests\n",
        "\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "_kq6_lmBpQqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to the TAP Server and run a query\n",
        "\n",
        "I took the code from here:  https://irsa.ipac.caltech.edu/docs/program_interface/TAP.html\n",
        "\n",
        "Then, in an act of desperation, I got ChatGPT to fix it for me.\n",
        "\n",
        "I just said, \"convert this into something I can use in google colab in python:\"\n",
        "\n",
        "`` curl -o fp_psc.tbl -F \"UPLOAD=my_table,param:table.tbl\" -F \"table.tbl=@upload.tbl\" -F \"FORMAT=IPAC_TABLE\" -F \"QUERY=SELECT fp_psc.ra, fp_psc.dec FROM fp_psc WHERE CONTAINS(POINT(ra,dec), CIRCLE(TAP_UPLOAD.my_table.my_ra, TAP_UPLOAD.my_table.my_dec, 0.01)) =1\" https://irsa.ipac.caltech.edu/TAP/sync ``\n",
        "\n",
        "Now this one in particular is running a query against the 2MASS catalog (formatting it as code so you don't have to run it, but you can always run it for funzies/to test).\n"
      ],
      "metadata": {
        "id": "0wrLOXOtp_ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "url = 'https://irsa.ipac.caltech.edu/TAP/sync'\n",
        "query = (\n",
        "    \"SELECT fp_psc.ra, fp_psc.dec \"\n",
        "    \"FROM fp_psc \"\n",
        "    \"WHERE CONTAINS(POINT(ra,dec), CIRCLE(TAP_UPLOAD.my_table.ra, TAP_UPLOAD.my_table.dec, 2.0/3600.)) = 1\"\n",
        ")\n",
        "\n",
        "files = {\n",
        "    'UPLOAD': 'my_table,param:table.tbl',\n",
        "    'table.tbl': open(\"/content/drive/My Drive/Colab Notebooks/gj.tbl\", 'rb')\n",
        "}\n",
        "\n",
        "funData = {\n",
        "    'FORMAT': 'IPAC_TABLE',\n",
        "    'QUERY': query\n",
        "}\n",
        "\n",
        "response = requests.post(url, files=files, data=funData)\n",
        "if response.status_code == 200:\n",
        "    response_content = response.content\n",
        "    with open(\"/content/drive/My Drive/Colab Notebooks/fun_fp_psc_output.tbl\", 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(\"File downloaded successfully.\")\n",
        "else:\n",
        "    print(f\"Error: {response.status_code}\")\n",
        "\n",
        "print(response_content)\n",
        "```"
      ],
      "metadata": {
        "id": "zbHr_q5_rOct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# URL of the raw file on GitHub\n",
        "url = \"https://raw.githubusercontent.com/lonestarchaser/SED/main/gj.tbl\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url)\n",
        "with open(\"gj.tbl\", 'wb') as file:\n",
        "    file.write(response.content)"
      ],
      "metadata": {
        "id": "fR71wrvmfNLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So let's adjust that code and make it work with Gaia!\n",
        "\n",
        "Go to this website:\n",
        "https://www.ipac.caltech.edu/dois/irsa\n",
        "\n",
        "Click on the database you want...\n",
        "\n",
        "And then in the hyperlink on the next page you can see what the catalog is called.\n",
        "\n",
        "https://irsa.ipac.caltech.edu/cgi-bin/Gator/nph-dd?catalog=gaia_edr3_source\n",
        "\n",
        "So I'll use ``gaia_edr3_source``\n",
        "\n",
        "....... Except now in the update it looks like they're hiding that information....\n",
        "\n",
        "Fun fact:\n",
        "\n",
        "IRSA says there should be 654 non-null entries, but it's only doing the closest match.  Here, I'm getting potentially multiple matches per item, so I get 699.\n",
        "\n",
        "To do math later we need just these columns:\n",
        "* ra: Right ascension\n",
        "* dec: Declination\n",
        "* parallax: Parallax\n",
        "* phot_g_mean_mag: G-band mean magnitude\n",
        "* bp_rp: BP - RP color\n",
        "* phot_bp_mean_mag: Integrated BP mean magnitude\n",
        "* phot_rp_mean_mag: Integrated RP mean magnitude\n"
      ],
      "metadata": {
        "id": "nUPmbyhMp5tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://irsa.ipac.caltech.edu/TAP/sync'\n",
        "query = (\n",
        "    \"SELECT TAP_UPLOAD.my_table.name, g.ra, g.dec, g.parallax, g.phot_g_mean_mag, g.bp_rp, g.phot_bp_mean_mag, g.phot_rp_mean_mag \"\n",
        "    \"FROM gaia_edr3_source AS g \"\n",
        "    \"WHERE CONTAINS(POINT(ra,dec), CIRCLE(TAP_UPLOAD.my_table.ra, TAP_UPLOAD.my_table.dec, 2.0/3600.0)) = 1\"\n",
        ")\n",
        "\n",
        "# Download the gj.tbl file from GitHub\n",
        "gj_url = \"https://raw.githubusercontent.com/lonestarchaser/SED/main/gj.tbl\"\n",
        "response_gj = requests.get(gj_url)\n",
        "with open(\"gj.tbl\", 'wb') as file:\n",
        "    file.write(response_gj.content)\n",
        "\n",
        "# Prepare the files dictionary\n",
        "files = {\n",
        "    'UPLOAD': 'my_table,param:table.tbl',\n",
        "    'table.tbl': open(\"gj.tbl\", 'rb')\n",
        "}\n",
        "\n",
        "funData = {\n",
        "    'FORMAT': 'IPAC_TABLE',\n",
        "    'QUERY': query\n",
        "}\n",
        "\n",
        "response = requests.post(url, files=files, data=funData)\n",
        "if response.status_code == 200:\n",
        "    ILikePuppies = response.content\n",
        "    print(\"Data retrieved successfully.\")\n",
        "else:\n",
        "    print(f\"Error: {response.status_code}\")\n",
        "\n",
        "# Print the content stored in ILikePuppies (optional)\n",
        "print(ILikePuppies)\n"
      ],
      "metadata": {
        "id": "ZOu7uqh9qiyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in a file from Google Drive\n",
        "\n",
        "We'll read in the file, using a fixed width column. Unfortunately, it's annoying to figure out how wide the columns are so we had to go through a process to figure that out.\n"
      ],
      "metadata": {
        "id": "KDQNzPVnho2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the index-th line from the variable that has the headers in it\n",
        "def get_this_line(file_content, index):\n",
        "    try:\n",
        "        lines = file_content.splitlines()\n",
        "        this_line = lines[index]\n",
        "        return this_line\n",
        "    except IndexError:\n",
        "        return f\"Error: The line index {index} is out of range.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "# Now look for where the pipes \"|\" are in that line\n",
        "def find_pipe_locations(line):\n",
        "    if isinstance(line, bytes):\n",
        "        line = line.decode('utf-8')  # Assuming UTF-8 encoding\n",
        "    return [index for index, char in enumerate(line) if char == '|']\n",
        "\n",
        "# Then create the column widths based on the locations of those.\n",
        "def create_tuples_from_indices(indices):\n",
        "    result = list(zip(indices, indices[1:]))  # Pair consecutive elements\n",
        "    return result"
      ],
      "metadata": {
        "id": "cz6u-ZIXhpca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now figure out where the individual columns for this file are."
      ],
      "metadata": {
        "id": "JJcR-MhFhxxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titles_start_this_line = 18\n",
        "\n",
        "this_line = get_this_line(ILikePuppies, titles_start_this_line)\n",
        "print(this_line)\n",
        "pipe_locations = find_pipe_locations(this_line)\n",
        "print(pipe_locations)\n",
        "myColSpecs = create_tuples_from_indices(pipe_locations)\n",
        "print(myColSpecs)"
      ],
      "metadata": {
        "id": "QFVy6vuUhyAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have each of the individual column widths, we can go ahead and pull that now.\n",
        "\n",
        "If you'd like to see the first 1000 (or whatever) rows, you can do this afterwards:\n",
        "\n",
        "`` data.head(1000) ``"
      ],
      "metadata": {
        "id": "tn9OUqLbjF9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert bytes to string\n",
        "ILikePuppies_str = ILikePuppies.decode('utf-8')  # Adjust encoding\n",
        "\n",
        "data = pd.read_fwf(StringIO(ILikePuppies_str), skiprows=titles_start_this_line+4, colspecs = myColSpecs, headers=None, names=[\"name\",\"ra\",\"dec\",\"parallax\",\"phot_g_mean_mag\",\"bp_rp\",\"phot_bp_mean_mag\",\"phot_rp_mean_mag\"])"
      ],
      "metadata": {
        "id": "r1MbIwPYjGM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Omg it's time to plot Color vs Absolute Magnitude Diagram!\n",
        "\n",
        "We're going to make this happen!\n",
        "\n",
        "* The x axis is B-R (``bp_rp``)\n",
        "* The y axis is the G-band Magnitude (``data.phot_g_mean_mag``)\n",
        "\n",
        "We'll plot the x and y values (with the y-axis flipped), but we need to correct apparent versus absolute magnitude.\n",
        "\n",
        "Recall the Magnitude - Distance Formula - used to give the relationship between the apparent magnitude, the absolute magnitude and the distance of objects.\n",
        "\n",
        "$$m - M = -5 + 5 \\log_{10}{d}$$\n",
        "\n",
        "Where:\n",
        "* $m$ = apparent magnitude\n",
        "* $M$ = absolute magnitude\n",
        "* $d$ = distance measured in parsecs (pc)\n",
        "\n",
        "Solving for $M$, we get\n",
        "\n",
        "$$M = m + 5 - 5 \\log_{10}{d}$$\n",
        "\n",
        "But we don't have parsecs, we have parallax, measured in milliarcseconds, so we need to do a quick conversion $$d = 1000/\\mathrm{parallax} $$\n",
        "\n",
        "Putting this into our formula, we get\n",
        "\n",
        "$$M = m + 5 - 5 \\log_{10}{(1000/\\mathrm{parallax})}$$\n",
        "\n",
        "Now, this is going to give us some weird outliers (I'm thinking because of computational numerical stuff with logarithms on extreme numbers), so we're going to limit our y-axis to the +/- 1.5 Interquartile Range (IQR).\n",
        "\n",
        "Since nothing is ever easy, there are definitely some NaNs in our data, so we have to use special functions to get statistics while ignoring NaNs.\n",
        "\n",
        "Also, in case you ever need it:\n",
        "* ``yMean = np.nanmean(y)     # excludes NaN``\n",
        "* ``yMedian = np.nanmedian(y) # excludes NaN``\n"
      ],
      "metadata": {
        "id": "5Co6e6ji2Usw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = data.bp_rp\n",
        "#y = data.phot_g_mean_mag\n",
        "y = data.phot_g_mean_mag - (5*np.log10(1000/data.parallax) - 5)\n",
        "plt.scatter(x,y,s = 2)\n",
        "plt.title('Super amazing picture')\n",
        "plt.xlabel('B - R')\n",
        "plt.ylabel('Absolute Magnitude')\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "# Calculate the 25% and 75% quantiles\n",
        "y25 = np.nanquantile(y,.25)\n",
        "y75 = np.nanquantile(y,.75)\n",
        "\n",
        "# Calculate the interquantile range\n",
        "IQR = y75 - y25\n",
        "\n",
        "# Then (kinda) arbitrarily set the limits on the plot\n",
        "myYMin = y25 - 1.5*IQR\n",
        "myYMax = y75 + 1.5*IQR\n",
        "\n",
        "ax.set_ylim([myYMin, myYMax])\n",
        "\n",
        "ax.invert_yaxis()"
      ],
      "metadata": {
        "id": "PPkEka1vxG0l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}